---
title: 'The Adolescent Brain Cognitive Development Study: Longitudinal Design and Analyses'
authors:
  - name: Samuel W. Hawes
    department: Department of Psychology; Center for Children and Families
    affiliation: Florida International University
    location: Miami, FL 33199
    email: shawes@fiu.edu
  - name: Andrew Littlefield
    department: Psychological Science
    affiliation: Texas Tech University
    location: Lubbock, TX 79409
    email: Andrew.Littlefield@ttu.edu
  - name: Daniel Lopez
    department: Division of Epidemiology, Department of Public Health Sciences
    affiliation: University of Rochester Medical Center
    location: Rochester, NY 14642
    email: Daniel_Lopez2@URMC.Rochester.edu
  - name: Kenneth J. Sher
    department: Department of Psychology
    affiliation: University of Missouri
    location: Columbia, MO 65211
    email: SherK@missouri.edu
  - name: tbd
    department: tbd
    affiliation: tbd
    location: tbd
    email: tbd
  - name: Wesley K. Thompson
    department: Population Neuroscience and Genetics Lab
    affiliation: University of California, San Diego
    location: San Diego, CA 92093
    email: wkthompson@ucsd.edu
    thanks: Use footnote for providing further information about author (webpage, alternative address)---*not* for acknowledging funding agencies. Optional.
abstract: |
  This is the abstract.
  This is the abstract. This is the abstract.
  This is the abstract. This is the abstract. This is the abstract. 
  This is the abstract. This is the abstract. This is the abstract.
  This is the abstract. This is the abstract. This is the abstract. 
  This is the abstract. This is the abstract. This is the abstract.
  This is the abstract. This is the abstract. This is the abstract. 
  This is the abstract. This is the abstract. This is the abstract.
  This is the abstract. This is the abstract. This is the abstract.
output:
  bookdown::pdf_book:
    base_format: rticles::arxiv_article
    keep_tex: false
    toc: true #table of contents
    toc_depth: 4  #
mainfont: Arial
link-citations: true
bibliography: references.bib
biblio-style: unsrt
keywords:
- keyword1
- keyword2
- (optional and can be removed)
header-includes:
- \usepackage{amsmath}
- \newcommand{\abcd}{Adolescent Brain Cognitive Development}
- \newcommand{\R}{\textsf{R}}
---

```{r setup, include=FALSE}
options(width=20)
knitr::opts_chunk$set(echo = TRUE, comment="")
```

---
<!-- NOTES


each method is presented according to the following triplet: (1) when to apply it and what is it doing, (2) how to compute it in R, and (3) how to present, visualize, and interpret the results. [@Mair]
-->
---

### Packages

```{r libraries, message=FALSE, warning=FALSE, echo = FALSE}
library(tidyverse) # xxxxxx
library(lavaan)
library(haven)
library(DiagrammeR)
library(knitr)
```

------------------------------------------------------------------------

```{=html}
<!-- 
# Prologue 







-->
```
# Introduction
The Adolescent Brain Cognitive Development^SM^ (ABCD) Study is the largest long-term investigation of neurodevelopment and child health in the United States. Conceived and initiated by the National Institutes of Health (NIH), this landmark prospective longitudinal study aims to transform our understanding of the genetic and environmental influences on brain development and their roles in behavioral and health outcomes in adolescents [@volkow2018]. At its heart, the study is designed to chart the course of human development across multiple, interacting domains from late childhood to early adulthood and to identify factors that lead to both positive and negative developmental outcomes. Central to achieving these goals is the ABCD Study's^®^ committement to an open science framework designed to facilitate access to and sharing of scientific knowledge by espousing practices that increase openness, integrity, and reproducibility of scientific research (e.g., public data releases). In this sense, the ABCD Study^®^ is a collaboration with the larger research community, with the rich longitudinal nature of the ABCD Study dataset allowing researchers to perform a variety of analyses of both methodological and substantive interest. Together, this presents a unique opportunity to significantly advance our understanding of how a multitude of biopsychosocial processes emerge and unfold across critical periods of development.

## The ABCD Study^®^ Data
A total cohort of $n = 11880$ US children aged 9-10 years at baseline (born between 2006-2008) and their parents/guardians was recruited from 22 sites (with one site no longer active). Eligible children were recruited from the household populations in defined catchment areas for each of the study sites during the roughly two-year period beginning September 2016 and ending in October 2018.(Information regarding funding agencies, recruitment sites, investigators, and project organization can be obtained at the [ABCD Study website](https://abcdstudy.org).

The ABCD Study is collecting longitudinal data on a rich variety of outcomes that that will enable the construction of realistically-complex etiological models by incorporating factors from many domains simultaneously. Each new wave of data collection provides the building blocks for conducting probing longitudinal analyses that allow us to characterize normative development, identify variables that presage deviations from prototypic development, and assess a range of outcomes associated with variables of interest. This data includes a neurocognitive battery [@luciana2018a; @thompson2019], mental and physical health assessments [@barch2018], measures of culture and environment [@zucker2018], substance use [@xxxxx], biospecimens [@uban2018], structural and functional brain imaging [@casey2018; @hagler2019], geolocation-based environmental exposure data, wearables and mobile technology [@bagot2018], and whole genome genotyping [@loughnan2020]. Many of these measures are collected at in-person annual visits, with brain imaging collected at baseline and at every other year going forward. A limited number of assessments are collected in semi-annual telephone interviews between in-person visits. Data are publicly released on an annual basis through the NIMH Data Archive [NIMH Data Archive; NDA](https://nda.nih.gov/abcd). By necessity, the study's earliest data releases were cross-sectional (i.e., the baseline data), however, the most recent public data release (NDA Release 4.0) contains data collected across three annual assessments, including two imaging assessments (baseline and year 2 follow-up visits).

## Organization of current manuscript

The rich longitudinal nature of the ABCD Study dataset will allow researchers to perform a number of analyses of both methodological and substantive interest. This report describes methods for longitudinal analyses of ABCD Study data that can address its fundamental scientific aims, as well as challenges inherent in a large population-based long-term study of adolescents. The manuscript is organized as follows: xxxxxxxxx.

***
# Part I: Longitudinal Research: Basic Concepts and Considerations 
There are a number of important concepts to consider when conducting longitudinal analyses in a developmental context. These include different ways of thinking about developmental course, whether certain periods of development are relatively sensitive or insensitive to various types of insults or stressors, whether some time periods or situations inhibit the expression of individual differences due to extreme environmental pressures, and whether the same behavior manifested at different times represent the same phenomenon or different ones. Further, in the case of developmentally focused longitudinal research, each new measurement occasion not only provides a more extended portrait of the child's life course (and not just characterize growth during this period but also assess the durability/chronicity of prior effects/consequences), but also brings with it greater methodological opportunities to exploit the statistical properties of longitudinal data in the furtherance of critical scientific questions. That is, we can ask more nuanced questions and make stronger inferences as our number of time-ordered observations grow, assuming we have assessed the "right" variables and the timings of our observations comport with the temporal dynamics of the mechanisms of interest. Appreciation of these and other issues can help to guide analysis and interpretation of data and aid translation to clinical and public health applications.                                           
<br>
## Vulnerable periods 
Development normatively progresses from less mature to more mature levels of functioning. However, unique epochs and experiences can alter the course of this idealized form of development. Consider research that shows cannabis use during adolescence is associated with later psychosis to a greater degree than cannabis use initiated later in development [@xxxx]; or, similarly, experimental research on rodents that shows rodent brains to be especially sensitive to the neurotoxic effects of alcohol on brain structure and learning early in development (corresponding to early adolescence in humans)[@xxxx]. These examples highlight the importance of considering the role of *vulnerable periods* -- temporal windows of rapid brain development or remodeling during which the effects of environmental stimuli (e.g. cannabis exposure) on the developing brain may be particularly pronounced-- when trying to establish an accurate understanding of the association between exposures and outcomes. 
<br>
## Developmental disturbances 
Whereas vulnerable periods heighten neurobiological susceptibility to environmental influences, at other times environmental pressures will tend to suppress stability and disrupt the orderly stochastic process of normative development (e.g., xxxxxx). This situation reflects a *developmental disturbance* in that the normal course of development is "disturbed" for a period of time by some time-limited process. In such cases, we might find that prediction of behavior in the period of the disturbance is reduced and/or, similarly, behavior exhibited during the disturbance might have less predictive power with respect to distal outcomes compared to behavior exhibited prior to and following the disrupted period. That is, once the environmental stimuli is removed (or the individual is removed from the environment) individual differences are again more freely expressed and the autoregressive effects increase to levels similar to those prior to entering the environment.
<br>
## Developmental snares and cascade effects 
Normative development can also be upended by experiences (e.g., drug use) that, through various mechanisms, disrupts the normal flow of development wherein each stage establishes a platform for the next. For example, substance use could lead to association with deviant peers, precluding opportunities for learning various adapative skills and prosocial behaviors, in effect, creating a "snare" that retards psychosocial development. Relatedly, the consequences of these types of events can cascade (e.g.,. school dropout, involvement in the criminal justice system) so that the effects of the snare are amplified. Although conceptually distinct from vulnerable periods, both of these types of developmental considerations highlight the importance of viewing behavior in the context of development and the importance of attempting to determine how various developmental pathways unfold.
<br>
### Distinguishing developmental change from experience effects 
One can often observe systematic changes over time in a variable of interest and assume this change is attributable to development. For example, cognitive abilities (e.g, verbal ability, problem solving) normatively grow earlier in development and often decline in late life (e.g., memory, speed of processing). However, the observed patterns of growth and decline often differ between cross-sectional vs. longitudinal effects [@salthouse2014] where subjects gain increasing experience with the assessment with each successive measurement occasion. Such experience effects on cognitive functioning have been demonstrated in adolescent longitudinal samples similar to ABCD [@sullivan2017] and highlight the need to consider these effects and address them analytically. In the case of performance-based measures [e.g., matrix reasoning related to neurocognitive functioning; see @salthouse2014], this can be due to "learning" the task from previous test administrations (e.g., someone taking the test a second time performs better than they did the first time simply as a function of having taken it before). Even in the case of non-performance-based measures (e.g., levels of depression), where one cannot easily make the argument that one has acquired some task-specific skill through learning, it has been observed that respondents tend to endorse lower levels on subsequent assessments [e.g., @beck1961; see @french2010a] and this phenomenon has been well documented in research on structured diagnostic interviews [@robins1985]. While it is typically assumed that individuals are rescinding or telling us less information on follow-up interviews, there is reason to suspect that in some cases the initial assessment may be artefactually elevated [see @shrout2018a]. Some designs (specifically, accelerated longitudinal designs) are especially well suited for discovering these effects and modelling them. While ABCD was not designed as an accelerated longitudinal design, the variability in age at the time of baseline recruitment (9 years, 0 months to 10 years, 11 months) allows some measures, collected on a yearly basis, to be conceptualized as an accelerated longitudinal design. Moreover, it is possible that in later waves, patterns of longitudinal missing data will allow some analyses to assess the confounded effects of age and number of prior assessments. However, ABCD is fundamentally a single-cohort, longitudinal design, where number of prior assessments and age are highly confounded and for, perhaps, most analyses, the possible influence of experience effects need to be kept in mind. |
<br>
# Part II Longitudinal Data: Interpretation / Issues / Pitfalls & Assumption
## Defining Features of Longitudinal Data Analysis (section intro)
The hallmark characteristic of longitudinal data analysis is its application to repeated assessments of the same assessment targets (e.g., individuals, families) across time.

While the primary reason for collecting longitudinal data is in pursuit of addressing scientific questions, from a methodological perspective, having multiple observations over time allows researchers to identify potentially problematic observations when highly improbable longitudinal patterns are observed.

That is, we can ask more nuanced questions and make stronger inferences as our number of time-ordered observations grow assuming we have assessed the "right" variables and the timings of our observations comport with the temporal dynamics of the mechanisms of interest.


## Modeling Data Across Two Time Points versus Three or More Time Points
Although the clear leap to the realm of longitudinal data involves going from one assessment to two or more assessments, there are also notable distinctions in designs based on two-assessment points versus three or more measurement occasions. Just as cross-sectional data can be informative in some situations, two waves of data can be beneficial in contexts such as when experimental manipulation is involved (e.g., pre/post tests), or if the central goal is prediction (e.g., trying to predict scores on Variable A at time T as a function of prior scores on Variable A and Variable B at time T-1). At the same time, data based on two assessments are inherently limited on multiple fronts. As [@rogosa1982] noted approximately forty years ago, "Two waves of data are better than one, but maybe not much better". These sentiments are reflected in more contemporary recommendations regarding best-practice guidelines for prospective data, which increasingly emphasize the benefits of additional measurement occasions for model identification and accurate parameter estimation. For example, @duncan2009 recommend that developmental studies include three or more assessment points, given it is impossible for data based on two time points to determine the shape of development (since linear, straight line change is the only possible form, given two assessments). Research designs that include three or more time points allow for increasingly nuanced analyses that more adequately tease apart sources of variation and covariation among the repeated assessments [@king2018]-- a key aspect of inferential research. For example, developmental theories are typically interested in understanding patterns of *within-individual* change over time (discussed in further detail, below); however, two data points provide meager information on change at the person-level. This point is further underscored in a recent review of statistical models commonly touted as distinguishing *within-individual* vs *between-individual* sources of variance in which the study authors concluded "... researchers are limited when attempting to differentiate these sources of variation in psychological phenomenon when using two waves of data" and perhaps more concerning, "...the models discussed here do not offer a feasible way to overcome these inherent limitations" [@littlefield2021](p. 10). It is important to note, however, that despite the current focus on two-wave designs versus three or more assessment waves, garnering three assessment points is not a panacea for longitudinal modeling. Indeed, several contemporary longitudinal models designed to isolate *within-individual* variability [e.g., the Latent Curve Model with Structured Residuals; @curran2014a] require at least four assessments to parameterize fully and, more generally, increasingly accurate parameter estimates are obtained as more assessment occasions are used [@duncan2009].

## Types of stability and change
(request permission to adapt King et al. (2018), Table 1)
If one were to try to sum up what development in a living organism is exactly, one could plausibly argue it's the characterization of stability and change as the organism traverses the life course. There are a few different ways to think of stability (and change). Consider we measure the height of all youth in a 6th grade class, once in the fall at the beginning of the school year and once again in the spring at the end of the school year. A common first step may be to compare the class's average height values obtained at these two different measurement occasions. This comparison of the average scores for the same group of individuals between different time points is referred to as "*mean-level*" stability as it provides information about continuity and change in the group level of an outcome of interest (e.g., height) over time. Another type of stability involves calculating the correlation between the values obtained at different time points (e.g., 'height in the fall' with 'height in the spring'). This type of "*rank-order*" stability evaluates between-individual change by focusing on the degree to which an individuals retain their relative placement in a group across time. For example, someone who is the shortest person in his class in 6th grade may grow considerably over the school year (i.e., exhibit mean level change), but still remain the shortest person among his classmates. That is, he is manifesting a type of rank order stability. Both types of stability and change are important. For example, mean-level change in certain traits might help to explain why, in general, individuals are particularly vulnerable to social influences at some ages more than others; rank order change might help to quantify the extent to which certain characteristics of the individual are more trait-like. For example, in some areas of development, there is considerable mean change that occurs over time (e.g., changes in Big 5 personality traits), but relatively high rank-order stability. Despite the useful information afforded by examining mean-level and rank-order change, these approaches are limited in that they provide little information about patterns of "*within-individual*" change and, in turn, can result in fundamental misinterpretations about substantial or meaningful changes in an outcome of interest.

There is a growing recognition that statistical models commonly applied to longitudinal data often fail to comport with the developmental theory they are being used to assess (e.g., Curran, Lee, Howard, Lane, & MacCallum, 2012; Hoffman, 2015; Littliefeld et al., 2021; xxxxx et al. (xxxx). Specifically, developmental studies typically involve the use of prospective data to inform theories that are concerned with clear *within-person* (i.e., intraindividual) processes (e.g., how phenotypes change or remain stable within individuals over time) [e.g., see @curran2011]. Despite this, methods generally unsuited for disaggregating between- and within-person effects (e.g., cross-lagged panel models [CLPM]) remain common within various extant literatures. As a result, experts increasingly caution of the need to xxxxxxxx [@xxxxx]. Fortunately, there exists a range of models that have been proposed to tease apart between- and within-person sources of variance across time [see @littlefield2021; @orth2021]. Most of these contemporary alternatives incorporate time-specific latent variables to capture between-person sources of variance and model within-person deviations around an individual's mean (or trait) level across time [e.g., RI-CLPM, @hamaker2015; LCM-SR, @curran2014a]. It is important to note however that these models require multiple assessments waves (e.g., four or more to fully specify the LCM-SR), additional expertise to overcome issues with model convergence, and appreciation of modeling assumptions when attempting to adjudicate among potential models in each research context [see @littlefield2021, for further discussion].




## Model Assumptions
(*include table/figure that matches analyses & assumptions)
Many statistical models assume some certain characteristics about the data to which they are being applied. For example, common assumptions of parametric statistical models include normality, linearity, and equality of variances. These assumptions must be carefully considered prior to conducting analysis so that valid inferences can be made from the data; that is, violation of a model's assumptions can substantively alter interpretation of results. Similarly, statistical models employed in the analyses of longitudinal data often entail a range of assumptions that must be closely inspected. One central issue for repeated measurements on an individual is how to account for the correlated nature of the data; another common feature of longitudinal data is heterogeneous variability; that is, the variance of the response changes over the duration of the study. Traditional techniques, such as a standard regression or ANOVA model, assumes residuals are independent and thus are inappropriate for designs that assess (for example) the same individuals across time. That is, given the residuals are no longer independent, the standard errors from the models are biased and can produce misleading inferential results. Although there are formal tests of independence for time series data (e.g., the Durbin-Watson statistic; Durbin & Watson, 1950), more commonly independence is assumed to be violated in study designs with repeated assessments. Therefore, an initial question to be addressed by a researcher analyzing prospective data is how to best model the [covariance structure] (**insert link**) of said data.

## Covariance Structures  
Statistical models for longitudinal data include two main components to account for assumptions that are commonly violated when working with repeated measures data: a model for the covariance among repeated measures (both the correlations among pairs of repeated measures on an individual and the variability of the responses on different occasions), coupled with a model for the mean response and its dependence on covariates (eg, treatment group in the context of clinical trials). This allows for the specification of a range of so-called covariance structures, each with its own set of tradeoffs between model fit and parsimony [e.g., see @kincaid2005]. 

## Accounting for Correlated Data
As an example, one alternative structure that attempts to handle the reality that correlations between repeated assessments tend to diminish across time is the autoregressive design. As the name implies, the structure assumes a subsequent measurement occasion (e.g., assessment at Wave 2) is regressed onto (that is, is predicted by) a prior measurement occasion (e.g., assessment at Wave 1). The most common type of autoregressive design is the AR(1), where assessments at time T + 1 are regressed on assessments at Time T. Identical to *compound symmetry*, this model assumes the variances are homogenous across time. Diverting from compound symmetry, this model assumes the correlations between repeated assessments decline exponentially across time rather than remaining constant. For example, per the AR(1) structure, if the correlation between Time 1 and Time 2 data is thought to be .5, then the correlation between Time 1 and Time 3 data would be assumed to be .5\*.5 = .25, and the correlation between Time 1 and Time 4 data would be assumed to be .5\*.5\*.5 = .125. As with compound symmetry, the basic AR(1) model is parsimonious in that it only requires two parameters (the variance of the assessments and the autoregressive coefficient). Notably, the assumption of constant autoregressive relations between assessments is often relaxed in commonly employed designs that use autoregressive modeling (e.g., cross-lagged panel models [CLPM]). These designs still typically assume an AR(1) process (e.g., it is sufficient to regress the Time 3 assessment onto the Time 2 assessment and is not necessary to also regress the Time 3 assessment onto the Time 1 assessments, which would result in an AR(2) process). However, the magnitude of these relations are often allowed to differ across different AR(1) pairs of assessment (e.g., the relation between Time 1 and Time 2 can be different than the relation between Time 2 and Time 3). These more commonly employed models also often relax the assumption of equal variances of the repeated assessments. Although the AR(1) structure may involve a more realistic set of assumptions compared to compound symmetry, in that the AR(1) model allows for diminishing correlations across time, the basic AR(1) model, as well as autoregressive models more generally, can also suffer from several limitations in contexts that are common in prospective designs. In particular, recent work demonstrates that if a construct being assessed prospectively across time is trait-like in nature, then autoregressive relations fail to adequately account for this trait-like structure, with the downstream consequence that estimates derived from models based on AR structures (such as the CLPM) can be misleading and fail to adequately demarcate between- vs. within-person sources of variance [@hamaker2015].

## Linear vs non-linear models
Identification of optimal statistcal models and appropriate mathematical functions requires an
understanding of the type of data being used. Repeated assessments can be based on either continuous or discrete measures. Examples of discrete measures include repeated assessments of binary variables (e.g., past 12-month alcohol use disorder status measured across a ten-year period), ordinal variables (e.g., a single item measuring level of agreement to a statement on a three-point scale including the categories of "disagree", "neutral", and "agree" in an ecological momentary assessment study that involves multiple daily assessments), and count variables (e.g., number of cigarettes smoked per day across a daily diary study). In many ways, the distributional assumptions of indicators used in longitudinal designs mirrors the decisions points and considerations when delineating across different types of discrete outcome variables, a topic which spans entire textbooks [e.g., see @lenz2016a]. For example, the Mplus manual [@muthen2017] includes examples of: a) censored and censored-inflated models, b) linear growth models for binary or ordinal variables, c) linear growth models for a count outcome assuming a Poisson model, d) linear growth models for a count outcome assuming a zero-inflated Poisson model, and e) discrete- and continuous-time survival analysis for a binary outcome. Beyond these highlighted examples, other distributions (e.g., negative binomial) can be assumed for the indicators when modeling longitudinal data. These models can account for issues that can occur when working with discrete outcomes, including overdispersion (when the variance is higher than would be expected based on a given distribution) and zero-inflation [when more zeros occur than is expected based on a given distribution; see @lenz2016]. Models involving zero-inflation parameters are referred to as two-part models, given one part of the model predicts the zero-inflation whereas the other part of the model predicts outcomes consistent with a given distribution [e.g., Poisson distribution; see @farewell2017, for a review of two-part models for longitudinal data]. Although there exist several alternative models for discrete indicators, some more recent models that have been proposed for prospective data are only feasible in cases where indicators are assumed to be continuous rather than discrete [e.g., latent curve models with structured residuals; @curran2014]. Given the sheer breadth of issues relevant to determining better models for discrete outcomes, it is not uncommon for texts on longitudinal data analysis to only cover models and approaches that assume continuous indicators [e.g., @little2013]. However, some textbooks on categorical data analysis provide more in-detailed coverage of the myriad issues and modeling choices to consider when working with discrete outcomes [e.g., @lenz2016, Chapter 11 for matched pair/two-assessment designs; Chapter 12 for marginal and transitional models for repeated designs, such as generalized estimating equations, and Chapter 13 for random effects models for discrete outcomes].


## Missing Data/Attrition
As recently reviewed by Littlefield (in press), investigators of prospective data are confronted with study attrition (i.e., participants may not provide data at a given wave of assessment) and thus approaches are needed to confront the issue of missing data. Three models of missingness are typically considered in the literature [see @little1989] . These three models are data: a) missing completely at random (MCAR), b) missing at random (MAR), and c) missing not at random (MNAR). Data that are MCAR means missing data is a random sample of all the types of participants (e.g., males) in a given dataset . MAR suggests conditionally missing at random [see @graham2009]. That is, MAR implies missingness is completely random (i.e., does not hinge on some unmeasured variables) once missingness has been adjusted by all available variables in a dataset (e.g., biological sex) . Data thar are MNAR are missing as a function of unobserved variables . @graham2009 provides an excellent and easy to digest overview of further details involving missing data considerations.

There are multiple approaches that have been posited to handle missing dat a. Before the advent of more contemporary approaches, common methods included several ad hoc procedure s. These include eliminating the data of participants with missing data (e.g., listwise or pairwise deletion) or using mean imputation (i.e., replacing the missing value with the mean score of the sample that did participate ). However, these methods are not recommended because they can contribute to biased parameter estimates and research conclusions [see @graham2009]. For example, last observation carried forward (LOCF) is a common approach to imputing missing data. LOCF replaces a participant's missing values after dropout with the last available measurement [@molnar2008]. This approach assumes stability (i.e., a given participants score is not anticipated to increase or decline after study drop out) and that the data are MCA R. However, as described by @molnar2008, it is common for treatment groups to show higher attrition compared to control groups in studies of dementia drug s. Given dementia worsens across time, using LOCF biases the results in favor of the treatment group [see @molnar2008, for more details].

More modern approaches, such as using maximum likelihood or multiple imputation to estimate missing data, are thought to avoid some of the biases of the older approaches [see @enders2010; @graham2009]. @graham2009 noted several "myths" regarding missing da ta. For example, Graham notes many assume the data must be minimally MAR to permit estimating procedures (such as maximum likelihood or multiple imputation) compared to other, more traditional approaches (e.g., using only complete case dat a). Violations of MAR impact both traditional and more modern data estimation procedures, though as noted by Graham, violations of MAR tend to have a greater effect on older metho ds. Graham thus suggests that estimating missing data is a better approach compared to the older procedures in most circumstances, regardless of the model of missingness [i.e., MCAR, MAR, MNAR; see @graham2009].

Attrition from a longitudinal panel study such as ABCD is inevitable and represents a threat to the validity of longitudinal analyses and cross-sectional analyses conducted at later time points, especially since attrition can only be expected to grow over time. While, to date, attrition in ABCD has been minimal (some cite here), it remains an important focus for longitudinal analysis and its significance is likely to only grow as the cohort ages. Ideally, one tries to minimize attrition through good retention practices from the outset via strategies designed to maintain engagement in the project [@cotter2005; @hill2016; @watson2018]. However, even the best executed studies need to anticipate growing attrition over the length of the study and implement analytic strategies design to provide the most valid inferences. Perhaps the most key concern with dealing with data missing due to attrition is assessing the degree of bias in retained variables attributable to attrition. Assuming that the data are not missing completely at random, attention to the nature of the missingness and employing techniques designed to mitigate attrition-related biases need to be considered in all longitudinal analyses. Several different approaches can be considered and employed depending upon the nature of the intended analyses and degree of missingness and variables available to help estimate variables.






<!-- *Quantifying effect sizes longitudinally*. I wasn’t sure about this section or what the subpoints were trying to get out – I know there was a lot written about ES in the paper you sent around so maybe some info there can be relevant here – I did use the same definitions as used in the other document. -->

## Quantifying effect sizes longitudinally
Given longitudinal data involve different sources of variance, quantifying effect sizes longitudinally is a more difficult task compared to deriving such estimates from cross-sectional data. Effect size can be defined as, "a population parameter (estimated in a sample) encapsulating the practical or clinical importance of a phenomenon under study." [@kraemer2014]. Common effect size metrics include r (i.e., the standardized covariance, or correlation, between two variables) and Cohen's d [@cohen1988]. Adjustments to common effect size calculations, such as Cohen's d, are required even when only two time points are considered [e.g., see @morris2002]. @wang2019a note there are multiple approaches to obtaining standardized within-person effects, and that commonly suggested approaches (e.g., global standardization) can be problematic [see @wang2019a, for more details]. Thus, obtaining effect size metrics based on standardized estimates that are relatively simple in cross-sectional data (such as r) becomes more complex in the context of prospective data. @feingold2009 noted that equations for effects sizes used in studies involving growth modeling analysis (e.g., latent growth curve modeling) were not mathematically equivalent, and the resulting effect sizes were not in the same metric as effect sizes from traditional analysis [see @feingold2009, for more details]. Given this issue, there have been various proposals to adjusting effect size measures in repeated assessments. @feingold2019 reviews approaches for effect size metrics for analyses based on growth modeling, including when considering linear and non-linear (i.e., quadratic) growth factors. @morris2002 review various equations for effect size calculations relevant to when combining estimates in meta-analysis with repeated measures and independent-groups designs. Other approaches to quantifying effect sizes longitudinally may be based on standardized estimates from models that more optimally disentangle between- and within-person sources of variance (as reviewed above). For example, within a RI-CLPM framework, standardized estimates between random intercepts (i.e., the correlation between two random intercepts for two different constructs assessed repeatedly) could be used to index the between-person relation, whereas standardized estimates among the structured residuals could be used as informing the effect sizes of within-person relations.





# Part III Taxonomy of Methods (to include worked examples, but not accompanying code)

## Traditional linear models used in the analysis of continuous longitudinal data

### Simple descriptive methods of analysis

### Change Scores

### Modeling group means over time (Analyzing Response Profiles)

#### Paired t-test

#### Univariate repeated-measures ANOVA

#### MANOVA

### Linear mixed-effects models

https://www.theanalysisfactor.com/extensions-general-linear-model/

#### Random effects vs. marginal model formulation

## Traditional non-linear models used in the analysis of categorical (non-Gaussian) longitudinal data

### Signed rank sum test (Wilcoxon)

### Marginal Models

#### Generalized Estimating Equations (GEE)
https://www.theanalysisfactor.com/extensions-general-linear-model/

### Generalized linear mixed-effects models (GLMMs)
https://www.theanalysisfactor.com/extensions-general-linear-model/

## Structural Equation Modeling (SEM) approaches to the analysis of longitudinal data

brief sem overview but mostly explained above

### Autoregressive and cross-lagged panel analysis (ARCL)

Rank-order stability can be used to model interindividual stability over time as an autoregressive process,. That is, the rank-order stability can be used to model continuity of a behavior. Assuming equal intervals, the correlations between T0 and T1, and between T1 and T2 could be used to estimate the correlation between T2 and T3.

Multilevel modeling approaches have often stressed methods to tease apart different sources of variance [e.g., distinguishing level-1, or within-person, variables from level-2, or between-person, variables in the context of prospective data; see @raudenbush2002]. However, models common in SEM, such as the CLPM, have been criticized extensively on the grounds this modeling approach cannot delineate between- and within-person sources of variation and covariations [see @littlefield2021, for a review]. Briefly, the CLPM assumes no stable between-person differences (beyond the implied relations among the auto-regressive paths) among the constructs that are assessed repeated ly. As noted by @hamaker2015, "...if stability of the constructs is to some extent of a trait-like, time-invariant nature, the inclusion of autoregressive parameters will fail to adequately control for this. As a result, the estimates of the cross-lagged regression coefficients will be biased, which may lead to erroneous conclusions regarding the underlying causal pattern." (p. 10 2). As demonstrated by @littlefield2021, the CLPM assumes relations identical to a series of two-predictor regression models in terms of estimating variances and covariances among measurements across ti me. As described above, regression models based on two waves of assessment are limited on several fronts, including being able to tease apart between- and within-person sources of variance. @hoffman2015 notes the cross-lagged estimate "smushes" the between- and within-person effects together as a single estima te. @berry2017a notes this results in CLPM estimates "...that are difficult (or impossible) to interpret meaningfully." (p. 118 7). Given these observations, and that prospective data are often used to inform theories involving clear within-person processes [e.g., see @curran2011], models that more optimally delineate between- and within-person sources of variance are need ed.

## What it is and when to use it

The cross lagged panel model (CLPM) is a flexible analytic approach that is commonly applied when researchers are interested in understanding how variables influence each other over time. This framework requires at least two variables assessed across two measurement occassions. Consider the most basic -- and eponymously named-- CLPM, which models the lagged associations between two variables (x, y). Let 'x1' and 'y1' denote variables x and y at time 1 and allow 'x2' and 'y2' denote variables x and y at time 2. This model compares the relative effects of x and y on each other ('x1-\>y2', 'y1-\>x2') (see Figure x) and can be easily extended to evaluate other hypothesized relationships. For example, evaluating contemporaneous associations between variables assessed during the same measurement occassion (e.g., 'x1 \<-\> y1'; 'x2 \<-\> y2'; see Figure x) is straightforward. Another common CLPM extension is the inclusion of autoregressive effects (e.g., 'x1-\>x2', 'y1-\>y2'; see Figure x). Also referred to as an autoregressive cross-lagged [ARCL] model, this extension provides an estimate of a variable's temporal (i.e., rank order) stability over time. The larger the value of the autoregressive coefficient (closer to 1) the more stable (greater % of variance explained in) the variable across measurement occasions. When contemporaneous (e.g., x1 \<-\> y1) and autoregressive effects (e.g., x1 -\> x2) are included, the standardized cross-lagged parameters of an ARCL model are often inferred as representing the bidirectional 'between-person' effects of 'x1-\>y2' (when controlling for y1) and 'y1-\>x2' (when controlling for x1).

One advantage of longitudinal analyses with multiple waves of data collection is that such processes can be modeled over time. Understanding transactional effects effects can be useful in devising strategies to intervene in various types of escalating cycles of violence such as child maltreatment [@cicchetti2006a) or the effects of intimate partner violence on child behavior problems [@chung2021]. Such processes could be modeled statistically in a number of ways by relying on estimating cross-lagged relations but require at least three times of measurement in order to distinguish beween-subject and within-sources of measurement [@hamaker2015; see discussion below).

## **APPENDIX CHAPTER 5 EXAMPLES: AUTOREGRESSIVE AND CROSSLAGED MODELS**

Using the xxxxx package, we evaluated the performance of the ARCL applied to longitudinal ABCD Study dataset to provide a practical example (all code available at [githublink]). For this example, we will consider the temporal association between youth report scores on the xxx (a measure of xxxxxxx) and yyy (a measure of xxxxxxx). First, we constructed a basic CLPM to simultaneously estimate the lagged association between xxxxxx (xt1) and subsequent yyyyy (yt2), as well as the lagged association between yyyyy (yt1) and subsequent xxxxx (xt2) (see single-headed arrow in Figure x, section xx). Results from this model reveal xxxxxxx (see Table x). However, the basic CLPM does not tell us anything about xxxx and assumes xxxxx.

### **CHAPTER 5 TOC**
* 5.1: Basic Crosslagged Panel Model
* 5.2: Crosslagged Panel Model w/contemporaneous effects
* 5.3: Autoregressive Crosslagged Model 

### EXAMPLE 5.1: Basic Cross-lag Panel Model (CLPM) with Continuous Indicators
***
The basic CLPM model does xxxxxxx.

    EXAMPLE 5.1 CODE 
    --- 
    TITLE: this is an example of a basic cross-lag panel model (only cross-lag parameters are modeled) with continuous indicators
    DATA: FILE IS example-data.dat; 
    VARIABLE: NAMES ARE x1-x2 y1-y2; 
    MODEL: y2 on x1;
           x2 on y1;

![EXAMPLE 5.1 Diagram. This is an example of a basic cross-lag panel model (only cross-lag parameters are modeled) with continuous indicators.](/Users/shawes/Desktop/outdoor house.png)

<br>

#### **EXAMPLE 5.1 CODE COMMENTARY**
    TITLE: this is an example of a CFA with continuous factor indicators
>   *The TITLE command is used to provide a title for the analysis. The title is printed in the output just before the Summary of Analysis.*
<br>

    DATA: FILE IS example-data.dat;
>   *The DATA command is used to provide information about the data set to be analyzed. The FILE option is used to specify the name of the file that contains the data to be analyzed, ex5.1.dat. Because the data set is in free format, the default, a FORMAT statement is not required.*
<br>

    VARIABLE: NAMES ARE x1-x2 y1-y2; 
>   *The VARIABLE command is used to provide information about the variables in the data set to be analyzed. The NAMES option is used to assign names to the variables in the data set. The data set in this example contains four variables: x1, x2, y1, y2.

<br>

    MODEL: y2 on x1;
           x2 on y1;
           
>   *The MODEL command is used to describe the model to be estimated. Here the two BY statements specify that f1 is measured by y1, y2, and y3, and f2 is measured by y4, y5, and y6. The metric of the factors is set automatically by the program by fixing the first factor loading in each BY statement to 1. This option can be overridden. The intercepts and residual variances of the factor indicators are estimated and the residuals are not correlated as the default. The variances of the factors are estimated as the default. The factors are correlated as the default because they are independent (exogenous) variables. The default estimator for this type of analysis is maximum likelihood. The ESTIMATOR option of the ANALYSIS command can be used to select a different estimator.*

<br>

### EXAMPLE 5.2: Cross-lag Panel Model (CLPM), including contemporaneous effects, with Continuous Indicators
***
The CLPM model with included contemporaneous effects does xxxxxxx.

    EXAMPLE 5.2 CODE 
    --- 
    TITLE: this is an example of a cross-lag panel model including contemporaneous effects, with Continuous Indicators
    DATA: FILE IS example-data.dat; 
    VARIABLE: NAMES ARE x1-x2 y1-y2; 
    MODEL: y2 on x1;
           x2 on y1;
           x1 with y1;
           x2 with y2;

![EXAMPLE 5.2 Diagram. this is an example of a cross-lag panel model including contemporaneous effects, with Continuous Indicators.](/Users/shawes/Desktop/outdoor house.png)

<br>

#### **EXAMPLE 5.2 CODE COMMENTARY**
    TITLE: this is an example of a cross-lag panel model including contemporaneous effects, with Continuous Indicators
>   *The TITLE command is used to provide a title for the analysis. The title is printed in the output just before the Summary of Analysis.*
<br>

    DATA: FILE IS example-data.dat;
>   *The DATA command is used to provide information about the data set to be analyzed. The FILE option is used to specify the name of the file that contains the data to be analyzed, ex5.1.dat. Because the data set is in free format, the default, a FORMAT statement is not required.*
<br>

    VARIABLE: NAMES ARE x1-x2 y1-y2;
>   *The VARIABLE command is used to provide information about the variables in the data set to be analyzed. The NAMES option is used to assign names to the variables in the data set. The data set in this example contains six variables: y1, y2, y3, y4, y5, y6. Note that the hyphen can be used as a convenience feature in order to generate a list of names.*
<br>

    MODEL: y2 on x1;
           x2 on y1;
           x1 with y1;
           x2 with y2;
>   *The MODEL command is used to describe the model to be estimated. Here the two BY statements specify that f1 is measured by y1, y2, and y3, and f2 is measured by y4, y5, and y6. The metric of the factors is set automatically by the program by fixing the first factor loading in each BY statement to 1. This option can be overridden. The intercepts and residual variances of the factor indicators are estimated and the residuals are not correlated as the default. The variances of the factors are estimated as the default. The factors are correlated as the default because they are independent (exogenous) variables. The default estimator for this type of analysis is maximum likelihood. The ESTIMATOR option of the ANALYSIS command can be used to select a different estimator.*    

<br>

###  EXAMPLE 5.3: Autoregressive Cross-lag (ARCL) model, with Continuous Indicators
***
The ARCL model does xxxxxxx.

    EXAMPLE 5.3 CODE 
    --- 
    TITLE: this is an example of an Autoregressive Cross-lag (ARCL) model. This model extends on the CLPM presented in example 5.3 by including autoregressive (t1 -> t2) effects in the model, with continuous indicators
    DATA: FILE IS example-data.dat; 
    VARIABLE: NAMES ARE x1-x2 y1-y2; 
    MODEL: y2 on x1;
           x2 on y1;
           x1 with y1;
           x2 with y2;
           y2 on y1;
           x2 on x1;

![EXAMPLE 5.3 Diagram. this is an example of a cross-lag panel model including contemporaneous effects, with Continuous Indicators.](/Users/shawes/Desktop/outdoor house.png)

<br>

#### **EXAMPLE 5.3 CODE COMMENTARY**
    TITLE: this is an example of an Autoregressive Cross-lag (ARCL) model. This model extends on the CLPM presented in example 5.3 by including autoregressive (t1 -> t2) effects in the model, with continuous indicators
>   *The TITLE command is used to provide a title for the analysis. The title is printed in the output just before the Summary of Analysis.*
<br>

    DATA: FILE IS example-data.dat;
>   *The DATA command is used to provide information about the data set to be analyzed. The FILE option is used to specify the name of the file that contains the data to be analyzed, ex5.1.dat. Because the data set is in free format, the default, a FORMAT statement is not required.*
<br>

    VARIABLE: NAMES ARE x1-x2 y1-y2;
>   *The VARIABLE command is used to provide information about the variables in the data set to be analyzed. The NAMES option is used to assign names to the variables in the data set. The data set in this example contains six variables: y1, y2, y3, y4, y5, y6. Note that the hyphen can be used as a convenience feature in order to generate a list of names.*
<br>

    MODEL: y2 on x1;
           x2 on y1;
           x1 with y1;
           x2 with y2;
           y2 on y1;
           x2 on x1;
>   *The MODEL command is used to describe the model to be estimated. Here the two BY statements specify that f1 is measured by y1, y2, and y3, and f2 is measured by y4, y5, and y6. The metric of the factors is set automatically by the program by fixing the first factor loading in each BY statement to 1. This option can be overridden. The intercepts and residual variances of the factor indicators are estimated and the residuals are not correlated as the default. The variances of the factors are estimated as the default. The factors are correlated as the default because they are independent (exogenous) variables. The default estimator for this type of analysis is maximum likelihood. The ESTIMATOR option of the ANALYSIS command can be used to select a different estimator.*

***

The difference between this example and Example 5.1 is that the factor indicators are binary or ordered categorical (ordinal) variables instead of continuous variables.  The CATEGORICAL option is used to specify which dependent variables are treated as binary or ordered categorical (ordinal) variables in the model and its estimation.  In the example above, all six factor indicators are binary or ordered categorical variables.  The program determines the number of categories for each factor indicator.  The default estimator for this type of analysis is a robust weighted least squares estimator (Muthén, 1984; Muthén, du Toit, & Spisic, 1997).  With this estimator, probit regressions for the factor indicators regressed on the factors are estimated.  The ESTIMATOR option of the ANALYSIS command can be used to select a different estimator.  An explanation of the other commands can be found in Example 5.1. 

With maximum likelihood estimation, logistic regressions for the factor indicators regressed on the factors are estimated using a numerical integration algorithm.  This is shown in Example 5.5.  Note that numerical integration becomes increasingly more computationally demanding as the number of factors and the sample size increase.

> XXXX: xxxxxx <!-- xxxxxxx. -->

> XXXX: xxxxxx <!-- xxxxxxx. -->

> XXXX: xxxxxx <!-- xxxxxxx. -->

> XXXX: xxxxxx <!-- xxxxxxx. -->

```{=html}
<!-- 



TITLE:     this is an example of a CFA with continuous factor indicators
The TITLE command is used to provide a title for the analysis.  The title is printed in the output just before the Summary of Analysis. 

DATA:      FILE IS ex5.1.dat;

The DATA command is used to provide information about the data set to be analyzed.  The FILE option is used to specify the name of the file that contains the data to be analyzed, ex5.1.dat.  Because the data set is in free format, the default, a FORMAT statement is not required. 

VARIABLE:  NAMES ARE y1-y6;

The VARIABLE command is used to provide information about the variables in the data set to be analyzed.  The NAMES option is used to assign names to the variables in the data set.  The data set in this example contains six variables: y1, y2, y3, y4, y5, y6.  Note that the hyphen can be used as a convenience feature in order to generate a list of names.   

MODEL:     f1 BY y1-y3;

           f2 BY y4-y6;

The MODEL command is used to describe the model to be estimated.  Here the two BY statements specify that f1 is measured by y1, y2, and y3, and f2 is measured by y4, y5, and y6.  The metric of the factors is set automatically by the program by fixing the first factor loading in each BY statement to 1.  This option can be overridden.  The intercepts and residual variances of the factor indicators are estimated and the residuals are not correlated as the default.  The variances of the factors are estimated as the default.  The factors are correlated as the default because they are independent (exogenous) variables.  The default estimator for this type of analysis is maximum likelihood.  The ESTIMATOR option of the ANALYSIS command can be used to select a different estimator.   

-->
```
#### CLPM Summary

xxxxxxxxx

### Latent change score analysis (LCS)

### Latent growth curve modeling (LGCM)

LGM is a flexible modeling strategy that encompasses a set of statistical methods that allow for specifying and evaluating relationships among variables that have been assessed repeatedly across multiple measurement occasions (Preacher, et al. 2008). In its most basic form, LGM parameterizes a series of observations using two parameters, an intercept (often but not always the first observations of the series) and a slope. Each parameter has its own variance so that individual differences in level (at the point of the intercept) and rate of change (i.e., the slope) are captured. The LGM framework offers several noteworthy benefits compared to some other approaches common to longitudinal data analysis. For example, research focused on characterizing stability often resolves rank-order stability but doesn't consider mean level change. This more traditional analytic approach investigates between-individual or "rank-order" change by focusing on an individual's relative placement in a group across time (Hawes et al, 2018). While evaluating of rank-order stability can certainly offer useful insight, this approach provides little information regarding within-individual change (i.e., individual differences in slope). Additionally, LGM models are extremely flexible and can model both linear and a wide range of nonlinear slopes including slopes that follow polynomial trends, a discrete change in linear slope that occurs at a given point in time (e.g., spline), or can be relatively "free" and be fitted to model the observed variables as they appear. Note that, solely focusing on rank-order estimates can lead to fundamental misinterpretations such as concluding that a construct of interest is undergoing no substantial or meaningful change. For example, consider child A who takes part in a study and reports decreasing scores on a measure of anxiety across 4 measurement occasions. Now consider a hypothetical wherein all other youth taking part in the study exhibit similar decreases to their anxiety scores. In this instance, child A's "rank-order" in the group may not change. This would result in relatively high stability estimates despite notable within-individual change to child A's anxiety scores over time. The LGM framework effectively deals with this issue by allowing for an examination of both, intraindividual (within-person) change over time as well as interindividual (between-person) variability in these patterns of change. For many research questions, it is important to be able to resolve both normative change and variation in how individuals partake in that change. The implementation of an LGM framework can provide novel insight into continuity and change underlying phenomena of interest. A fundamental aspect of this approach is the ability to characterize the direction and rate of change at both, the group- and individual-level within a single model. In a conventional LGM a single trajectory that represents the overall group-mean is estimated, as well the individual variability around this mean trajectory. Further, extensions to the conventional LGM enable researchers to test more elaborate models. For example, theoretical accounts often hypothesize that different unobserved subpopulations of individuals in a population exhibit unique patterns of growth across development. More advanced LGM methods, including latent class growth analysis (LCGA) and growth mixture modeling (GMM), accommodate this increased complexity by estimating distinct trajectories that can differ in their direction and rate of change Kenny's comment: Add citation. In addition to modeling growth, LGM methods allow researchers to evaluate other substantive questions relevant to continuity and change, such as the influence of time-varying covariates, antecedent predictors and consequences of change, and unidirectional and transactional processes among multiple growth processes. To better demonstrate applications and advantages of the LGM framework we now turn to several worked examples using real-world data from the ABCD Study Curated Annual Release 2.0 (MPlus and R code for all examples is provided in supplemental documents). Example data come from the youth self-report version of the Brief Problem Monitor (BPM; citation). The BPM is a rating instrument used to monitor children's functioning and response to interventions. The measure includes 18-items scored on a 0 ("not true") to 2 ("very true") scale and is comprised of 3 subscales (Externalizing, Internalizing, Attention). BPM data used in the following examples was collected across 3 measurement occasions (i.e., 6-month, 1-year, and 18-month follow-up assessments) and only youth with complete BPM data at each assessment Kenny's comment: I might say, for purposes of illustrations as we don't want to encourage folks to routinely use listwise deletion were included in the worked examples (n = 1,864). All models were estimated using maximum likelihood estimation with robust standard errors (MLR). Model fit was assessed using several common fit indices (e.g., Comparative Fit Index [CFI; Hu & Bentler, 1999], Root Mean Square Error of Approximation [RMSEA; Browne, Cudeck, Bollen, & Long, 1993], sample-size adjusted Bayesian Information Criterion [BIC]. Example 1. The Conventional Latent Growth Model (LGM) A conventional LGM consists of repeated measurements of some variable 'y'. In this model two latent factors are specified to represent facets of change in y over time. The first latent factor, the intercept, represents y at baseline or when the time variable is equal to 0 (t = 0). The second latent factor, the slope, represents linear change in y across the repeated measurements. In a conventional LGM the following six parameters are estimated: 1) intercept mean (μi); 2) intercept variance (θi); 3) slope mean (μs); 4) slope variance (θs); 5) intercept and slope covariance (θis); and 6) an error variance that remains constant across repeated measurements (θε). We will now evaluate a conventional LGM that was estimated using BPM Externalizing scale scores obtained across 3 measurement occasions. An examination of the Comparative Fit Index (CFI) and root mean square error of approximation (RMSEA) reveals evidence of poor model fit (CFI = .91; RMSEA = .25). Although CFI was within the generally acceptable range (i.e., \>.90; citation), RMSEA was well above the generally recommended minimum cutoff (i.e., \<.10; citation). This suggests it is necessary to reevaluate our specified model. In the context of the current example however, we shall proceed as though the model demonstrated adequate fit. Observing the model parameter estimates shows the mean estimated intercept (μ = 1.86, SE = 0.04) and linear slope (μ = -.09, SE = 0.02) to be significant (ps \< .001). While the intercept factor provides information about initial status, some researchers suggest that this parameter is primarily meaningful for growth processes with a natural origin (Stoel, 2003; Stoel & van den Wittenboer, 2003). For the conventional LGM, the parameter that is most often of central interest is the slope factor. In the present example, the negative linear slope value indicates that there is a systematic mean-level decrease in BPM Externalizing scores across assessment waves (see Figure 1a). Importantly however, the variance estimate around this group-mean trajectory is statistically significant (θs = .38, p \< .001). This indicates that although BPM Externalizing scores are decreasing on average, there is significant within-individual variability in these patterns of change (see Figure 1b).

#### Univariate LGCM

#### Multivariate LGCM

##### Parallel process LGCM (PP-LGCM)

Example 3. Multivariate Latent Growth Models (Parallel Process Models) 8.2 Growth Processes (Kenny) Kenny's comment: In addition to being written, this should precede the LGM piece I think Potentially distinguishing two ostensibly similar phenotypes on the basis of differences in etiology (e.g., equifinality) 5) Tracking the range of different outcomes associated with a single process (e.g., multifinality) (this might be more of a substantive issue) Substantively, we are interested in variability of the trajectories of key risk factors and outcomes.

### Person-Centered Models

#### Latent transition analysis (LTA)

#### Latent class growth analysis (LCGA)

#### Growth mixture models (GMM)

Example 2. Growth Mixture Modeling (GMM) On some occasions the conventional LGM may be limited in that it describes all individuals as coming from the same population, not accounting for potentially important subgroup effects. An important extension of the conventional model, GMM allows different unobserved subgroups to follow distinct trajectories. By allowing the intercept and slope growth factors to vary across groups, the GMM approach estimates a distinct group-mean trajectory with its own unique variance, for each latent class. A special case of GMM, LCGA models a separate trajectory for each latent class, but the growth factor variances Kenny's comment: Intercept variances too, no? Clearer to say intercept and slope variances rather than growth factor? within each class are fixed to zero (i.e., assumes homogenous within-class patterns of growth). As this approach is less computationally expensive, it is often a useful starting approach for identifying different underlying trajectories prior conducting a GMM. Expanding on our prior example, we now examine a GMM using the same BPM Externalizing scale scores used in the conventional model. A successive number of latent classes were specified across models and the optimal number of classes were determined by investigating recommended model selection criteria (e.g., adjusted Bayesian Information Criterion [aBIC], Bootstrapped Likelihood Ratio Test [BLRT], parsimony, and interpretability). An overview of criteria used to select the optimal number of trajectory classes is provided in Table 3. These results show aBIC to decrease across iterations, however, BLRT suggests a 4-class solution offers the optimal fit. Further, examination of the 5-class model reveals two classes that comprise less than 5% of the sample (class 3 n = 11 [\< 1%]; class 4 n = 63 [3%]). Therefore, based on substantive interpretation, parsimony, and fit, we opt to select the four-class model of BPM Externalizing scores as providing the best overall solution. Estimated model parameters are presented in Table 4. Kenny's comment regarding previous sentence: This is one of those areas where some might argue there are too many investigator degrees of freedom. I also wonder whether there should be discussion of other issues like factor mixture models. nonlinear The Kenny's comment: would superimposition on a spaghetti plot (or four spaghetti plots) help illustrate the concept here. If model derived data are being plotted we might want to encourage folks to try to plot them in conjunctoin with raw data. results show a small (n = 64; 4%) group of youth with a persistently high BPM Externalizing trajectory and large group of youth (n = 1437; 77%) following a low and decreasing trajectory. The remaining two classes were characterized by youth with moderate initial scores that decreased across assessments (n = 191; 10%) and youth with moderate scores that remained stable over time (n = 171; 9%) (see Figure 2). Kenny's comment: I don't see "figure 2" here. Also, I might label groups as "Moderate-High decreasing and Moderate -Low Stable. I say there because the intercept mean between the two"moderates" is larget than between Moderate Decreasing and Persistent High and between Moderate Stable and Low decreasing.

##### Predictors and outcomes

##### 3-Step approach

### Additional SEM Extensions

#### State-Trait Models

#### RI-CLPM

Fortunately, there exists a range of models that have been proposed to distinguish between- and within-person relations across time [see @littlefield2021; @orth2021]. Most of these alternative models stem from latent state-trait models [@schmitt1993], where a latent variable (typically referred to as a "trait) captures between-person sources of variance and covariance among a series of repeated assessments across time. After accounting for this source of variance, relations among specific assessments across time (e.g., the relation between Construct 1 at Time 1 with Construct 2 at Time 2) can be modeled akin to the CLPM that are thought to capture "state" relations. More contemporary models, such as the Random-Intercept Cross-Lagged Panel Model [RI-CLPM; @hamaker2015] and the Latent Curve with Structured Residuals [LCM-SR; @curran2014a] utilize structured residuals, or time-specific latent variables that capture within-person deviations around an individual's mean (or trait) level across time. For example, when a random intercept (i.e., within a SEM framework, a latent variable where each assessment serves as an indicator and all factor loadings are constrained to unity) is added to a series of repeated assessments, the estimated score for a given individual at a given timepoint becomes a function of: 1) the mean at a given timeframe and 2) the latent variable score for a given individual. Any difference, or residual, between the actual scores and estimated score for a given individual at a given point of assessment can be thought of as within-person variance at that timepoint, and this within-person variance is captured by the structured residual [see @littlefield2021, for an example with actual data]. The RI-CLPM proposed by @hamaker2015] only includes a random intercept, whereas the LCM-SR proposed by @curran2014a also includes additional growth factors, such as a random linear slope (such that the LCM-SR could be considered as a hybrid between traditional latent growth modeling with autoregressive and cross-lagged relations among structured residuals). Variants of these models [see @littlefield2021, for more details], such as relaxing the assumption of unit factor loadings inherent in a random-intercept model, can also be estimated given an adequate number of assessments (typically four or more) are acquired [see @mulder2021 & @hamaker2020a for a description and @littlefield2021, for a worked example of the impact of these adjustments on model estimates].

It is important to realize the various assumptions inherent in these models [see @littlefield2021, for more details]. For example, a random-intercept only model assumes equivalent covariances among repeated assessments (as does compound symmetry described above), and the only way to relax this assumption is through the inclusion of relations among the structured residuals. Indeed, when the assumption of covariance stationarity holds (i.e., the assumption inherent in the oft-used repeated measures ANOVA), all the relations among the structured residuals become zero [see @littlefield2021, for a demonstration]. Conversely, the LCM-SR can relax the assumption of stationarity of covariances by the inclusion of additional growth factors (e.g., a random linear slope) in addition to the relations among structured residuals. Thus, as demonstrated in @littlefield2021, different relations among variables thought to capture within-person sources of variance can be obtained in the same data depending on the choice of growth factors (i.e., intercept-only vs. models that include additional growth factors) and if mean structure is also considered [see @littlefield2021]. Evidence also suggests that achieving model convergence is more difficult for some models (e.g., the LCM-SR) compared to other models (e.g., the RI-CLPM; see @orth2020) when modeling is based on default specifications. In sum, the importance of isolating the appropriate level of analysis in prospective data has been stressed by several researchers (particularly among Patrick Curran and colleagues within SEM frameworks). Despite this, models such as the CLPM remain common within various extant literatures. There are multiple alternatives that have been recommended, though these models require multiple assessments waves (e.g., four or more to fully specify the LCM-SR), additional expertise to overcome issues with model convergence, and appreciation of modeling assumptions when attempting to adjudicate among potential models in each research context [see @littlefield2021, for further discussion].

#### Latent Curve Models with Structural Residuals (LCM-SR)

multivariate Latent Curve Model with Structured Residuals (LCM-SR; Curran et al., 2014) (see Figure 1). The LCM-SR model is a novel methodology that extends current analytic procedures by allowing more precise investigation into estimates of person- specific, "between-person" and time-specific, "within-person" processes among distinct constructs over time (Curran et al., 2014). This model has the ability to simultaneously estimate the association between psychopathic features and alcohol use across development, while also isolating the between-person and within-person components of these processes over time (see Curran et al., 2014). The multivariate LCM-SR has similarities to other models that have also extended the more general latent curve model (LCM) framework, such as autoregressive latent trajectory (ALT; Curran & Bollen, 2001; Bollen & Curran, 2004) models. However, unlike previous methods, a major advantage of the LCM-SR framework is that it imposes a structure onto the time- specific residuals of the observed repeated measures for each process (e.g., psychopathic features and alcohol use). This structure results in these residuals being conceptualized as time-specific estimates of the deviation between the observed repeated measure and the underlying latent growth curve. This time-specific residual structure represents the within- person portion of the model (Curran et al., 2014). It is possible to incorporate both autoregressive and cross-lagged regressions in this within-person portion of the model,

LCM-SR models require a model building process that is carried out in a series of steps (Curran et al., 2014). More simple models are specified initially such as evaluating the fit of separate univariate growth models for each process (i.e., psychopathic features, alcohol use). Subsequent models increase in complexity and the ability of this added complexity to improve the model is evaluated at each step by examining the results Satorra-Bentler chi- square difference testing (Satorra & Bentler, 2001), as well as evaluation of additional fit indices (e.g., CFI, RMSEA, BIC, Wald tests).

In the current study, after finding each of our univariate growth curve models to provide a good fit to the data, our first step was to combine the trajectories of psychopathic features and alcohol use into a parallel process Latent Growth Curve Model (LGCM). Next we imposed a structure onto the time-specific residuals and specified a model consisting of autoregressive and cross-lagged parameters of this residual structure. In the subsequent step the full LCM-SR model was specified by combining the parallel process LGCM with the autoregressive and cross-lagged residual structure. Finally, we compared the fit of a series of models resulting from fixing and freeing parameters of the trajectory portion of the model (i.e., slope variance for alcohol use and psychopathic features) and the residual structure aspect of the model (i.e., concurrent time-specific correlations, autoregressions, and crosslags). All models were specified using maximum likelihood estimation with standard errors and a chi-square statistic that are robust to nonnormality (MLR) in Mplus 7 (Muthén & Muthén, 1998--2012) (also see Bollen & Curran, 2004, 2006; Curran et al., 2014; Morin, Maiano, Marsh, Janosz, and Nagengast, 2011).

##### Disaggregating within-person vs between-person effects


#### Time to Event Analyses







